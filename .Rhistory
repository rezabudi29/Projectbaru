neighbors <- neighbors[order(neighbors[,2]),]
# take the top k neighbors
neighbors <- neighbors[1:k,]
# determine the output and add this to predictions vector
if(nrow(neighbors[neighbors[,1] == "horeca", ]) > nrow(neighbors[neighbors[,1] == "retail", ])){pred.v <- c(pred.v, "horeca")
}else pred.v <- c(pred.v, "retail")
}
return(pred.v)
}
accuracy <- function(data){
# initialize number of predictions
correct <- 0
for(i in c(1:nrow(data))){
#7th variable is actual class, 8th is our predicted class
if(data[i, 7] == data[i, 8]){
correct <- correct + 1
}
}
percentage <- correct/nrow(data) *100
return(percentage)
}
predictions <- knn_classifier(train = wholesale_train, test = wholesale_test, k=19)
wholesale_test[,8] <- predictions
print(accuracy(wholesale_test))
CrossTable(x = wholesale_test$Industry, y = wholesale_test$V8)
horeca_pred <- knn(train = wholesale_train, test=wholesale_test, cl=wholesale_train$Industry, k=20)
horeca_pred <- knn(train = wholesale_train, test=wholesale_test[,-1], cl=wholesale_train$Industry, k=20)
horeca_pred <- knn(train = wholesale_train[,1:6], test=wholesale_test[,1:6], cl=wholesale_train$Industry, k=20)
length(horeca_pred)
wholesale_test[wholesale_test$Industry != horeca_pred, ]
table(horeca_pred, wholesale_test$Industry)
horeca_pred <- knn(train = wholesale_train[,1:6], test=wholesale_test[,1:6], cl=wholesale_train$Industry, k=18)
table(horeca_pred, wholesale_test$Industry)
horeca_pred <- knn(train = wholesale_train[,1:6], test=wholesale_test[,1:6], cl=wholesale_train$Industry, k=20)
table(horeca_pred, wholesale_test$Industry)
horeca_pred <- knn(train = wholesale_train[,1:6], test=wholesale_test[,1:6], cl=wholesale_train$Industry, k=20)
wholesale_test[wholesale_test$Industry != horeca_pred, ]
table(horeca_pred, wholesale_test$Industry)
horeca_pred <- knn(train = wholesale_train[,1:6], test=wholesale_test[,1:6], cl=wholesale_train$Industry, k=20)
wholesale_test[wholesale_test$Industry != horeca_pred, ]
set.seed(10)
horeca_pred <- knn(train = wholesale_train[,1:6], test=wholesale_test[,1:6], cl=wholesale_train$Industry, k=20)
set.seed(10)
horeca_pred <- knn(train = wholesale_train[,1:6], test=wholesale_test[,1:6], cl=wholesale_train$Industry, k=15)
horeca_pred <- knn(train = wholesale_train[,1:6], test=wholesale_test[,1:6], cl=wholesale_train$Industry, k=15)
wholesale_test[wholesale_test$Industry != horeca_pred, ]
table(horeca_pred, wholesale_test$Industry)
horeca_pred <- knn(train = wholesale_train[,1:6], test=wholesale_test[,1:6], cl=wholesale_train$Industry, k=15)
wholesale_test[wholesale_test$Industry != horeca_pred, ]
horeca_pred <- knn(train = wholesale_train[,1:6], test=wholesale_test[,1:6], cl=wholesale_train$Industry, k=15)
wholesale_test[wholesale_test$Industry != horeca_pred, ]
horeca_pred <- knn(train = wholesale_train[,1:6], test=wholesale_test[,1:6], cl=wholesale_train$Industry, k=15)
wholesale_test[wholesale_test$Industry != horeca_pred, ]
names(wholesale_test$V8)
name(wholesale_test$V8)
colname(wholesale_test$V8)
name(wholesale_test)
names(wholesale_test)
names(wholesale_test)
names(wholesale_test)
c(names(wholesale_test)[1:7], "ManualKNN")
names(wholesale_test)[8]
names(wholesale_test)[8] <- "ManualKNN"
horeca_pred <- knn(train = wholesale_train[,1:6], test=wholesale_test[,1:6], cl=wholesale_train$Industry, k=15)
wholesale_test$libkNN <- horeca_pred
wholesale_test[wholesale_test$Industry != horeca_pred, 7:9]
table(horeca_pred, wholesale_test$Industry)
loan_test <- read.csv("data_input/loan2017Q4.csv")
set.seed(2018)
loan_test <- loan_test[sample(nrow(loan_test), 1500),]
write.csv(loan_test, "lbb_loans.csv", row.names=F)
table(loan_test$not_paid)
?scale
summary(horeca_pred)
wholesale_lbb <- wholesale.n
wholesale_lbb$ishoreca <- ifelse(wholesale_lbb$Industry == "horeca", 1, 0)
wholesale_lbb <- wholesale_lbb[,-7]
lbb2 <- glm(ishoreca ~ ., wholesale_lbb, family="binomial")
summary(lbb2)
lbb2 <- glm(ishoreca ~ ., wholesale_lbb[1:339,], family="binomial")
summary(lbb2)
lbb2_pred <- predict(lbb2, wholesale_lbb[340:440,], type="response")
table("actual"= wholesale_lbb[340:440,7], "predicted"=lbb2_pred)
table("actual"= wholesale_lbb[340:440,7], "predicted"=as.numeric(lbb2_pred>=0.5))
lbb2 <- glm(ishoreca ~ ., wholesale_lbb[1:340,], family="binomial")
summary(lbb2)
lbb2_pred <- predict(lbb2, wholesale_lbb[341:440,], type="response")
table("actual"= wholesale_lbb[341:440,7], "predicted"=as.numeric(lbb2_pred>=0.5))
honors.logit <- glm(hon ~ 1, data=honors, family="binomial")
honors <- read.csv("data_input/sample.csv")
honors$hon <- as.factor(honors$hon)
prop.table(table(honors$hon))
honors.logit <- glm(hon ~ 1, data=honors, family="binomial")
summary(honors.logit)
honors.logit2 <- glm(hon ~ female, data=honors, family="binomial")
honors.logit3 <- glm( hon ~ math, data=honors, family="binomial" )
summary(honors.logit3)
honors.logit4 <- glm(hon ~ math + female + read, data=honors, family="binomial")
summary(honors.logit4)
honors.logit4 <- glm(hon ~ math + female + read, data=honors, family="binomial")
summary(honors.logit4)
honors.logit4 <- glm(hon ~ math + female + read, data=honors, family="binomial")
summary(honors.logit4)
honors.logit4 <- glm(hon ~ math + female + read, data=honors, family="binomial")
summary(honors.logit4)
honors.logit4 <- glm(hon ~ math + female + read, data=honors, family="binomial")
summary(honors.logit4)
honors.aic <- paste(honors.logit$aic, "\n", honors.logit2$aic, "\n", honors.logit3$aic, "\n", honors.logit4$aic, "\n", honors.logit5$aic, "\n")
honors.aic <- paste(honors.logit$aic, "\n", honors.logit2$aic, "\n", honors.logit3$aic, "\n", honors.logit4$aic, "\n", honors.logit5$aic, "\n")
honors.aic <- paste(honors.logit$aic, "\n", honors.logit2$aic, "\n", honors.logit3$aic, "\n", honors.logit4$aic, "\n")
cat(honors.aic)
honors.aic <- paste(honors.logit$aic, "\n", honors.logit2$aic, "\n", honors.logit3$aic, "\n", honors.logit4$aic)
cat(honors.aic)
AIC(honors.logit)
AIC(honors.logit4)
honors.null.resi <- paste(honors.logit$null.deviance, "\ ", honors.logit$deviance, "\n", honors.logit2$null.deviance, "\ ", honors.logit2$deviance, "\n", honors.logit3$null.deviance, "\ ", honors.logit3$deviance, "\n", honors.logit4$null.deviance, "\ ", honors.logit4$deviance, "\n")
cat(honors.null.resi)
honors.train <- honors[1:150,]
honors.test <- honors[151:200,]
honors.logit5 <- glm(hon ~ female + read + math + write, data=honors.train, family="binomial")
summary(honors.logit5)
Inf*-1000000 > 0.5
Inf-1000000 > 0.5
exp(28.99738)
exp(coef(honors.logit5[5]))
exp(coef(honors.logit5)[5])
4-1000000 > 0.5
Inf-1000000 > 0.5
exp(coef(honors.logit5)[5])
# Infinity minus any arbitarily large numbers still exceed 0.5
Inf-1000000 > 0.5
knitr::opts_chunk$set(cache=TRUE)
options(scipen = 9999)
library(dplyr)
library(grid)
library(gtools)
library(class)
library(gmodels)
install.packages("gmodels")
library(dplyr)
library(grid)
library(gtools)
library(class)
library(gmodels)
curve((x/(1-x)), from = 0, to=1, ylim=c(0,10), xlab = "Probability", ylab="Odds")
x <- 0.5
log(x/ (1-x))
# Plot sigmoid curve
curve(log(x/ (1-x)), from = 0, to=1, xlab = "Probability", ylab="Log of odds")
log 0
log (0)
log (-1)
1/0
library(gtools)
# Plot sigmoid curve
# logit(x) == log(x/(1-x))
curve(logit(x), from = 0, to=1, xlab = "Probability", ylab="Log of odds")
# Proof
## using logit(p, wo/ normalization)
proof1 <- logit(30, min=1, max=100)
## using logit(p, w/ min-max normalization)
proof2 <- logit(0.292929293)
## equivalent: log(p/(1-p))
proof3 <- log(0.292929293/(1-0.292929293))
proof1 <- round(proof1, 4)
proof2 <- round(proof2, 4)
proof3 <- round(proof3, 4)
print(paste(proof1, " | ", proof2, " | ", proof3))
library(gtools)
# Plot sigmoid curve
curve(inv.logit(x), from = -10, to=10)
honors <- read.csv("data_input/sample.csv")
honors$hon <- as.factor(honors$hon)
prop.table(table(honors$hon))
honors <- read.csv("data_input/sample.csv")
honors$hon <- as.factor(honors$hon)
prop.table(table(honors$hon))
honors <- read.csv("data_input/sample.csv")
honors$hon <- as.factor(honors$hon)
prop.table(table(honors$hon))
library(readr)
sample <- read_csv("data_input/sample.csv")
View(sample)
honors$hon <- as.factor(honors$hon)
str(honors)
prop.table(table(honors$hon))
honors.logit <- glm(hon ~ 1, data=honors, family="binomial") #family binomial digunakan karena distribusi binomial ingin menghitung probability dari variable dengan 2 kelas
summary(honors.logit) #coefficient menunjukkan hasil estimasi beta0, beta1. null deviance menunjukkan seberapa bagus hasil dari model yang dibangun.
#intrepretasi hasil di bawah: dari nilai nilai intercept -1.1255 yang menunjukkan bahwa odds nya adalah 0.3244, yang artinya probabilitas dia tidak dapat honor lebih besar dibandingkan peluang dia dapat honor
honors.logit <- (glm(hon ~1, data = honors, family = "binomial"))
summary(honors.logit)
exp(-1.12546)/(1+exp(-1.12546))
honors.logit2 <- glm(hon ~ female, data=honors, family="binomial")
table(honors = honors$hon, female = honors$female)
paste("Male:", (17/91)/(74/91))
paste("Male:", (17/91)/(74/91))
paste("Female:", (32/109)/(77/109))
#log(odds ratio)
log(0.4155844/0.2297297)
summary(honors.logit2)
(18/92) / (75/92)
log (0.24)
log((49/200)/(151/200))
log(0.3245033)
table(honors$hon)
prop.table(table(honors$hon))
(49/32) / (151/200)
log (49/32) / (151/200)
coefficients(honors.logit)
-1.50792 - (-1.66426)
# 0.15634, the diff with a one-unit increase should also be the
# coefficient for our math variable
honors.logit3$coefficients[2]
summary(honors.logit2)
loans.s <- read.csv("data_input/loan2017Q4.csv")
str(loans.s)
library(readr)
loan2017Q4 <- read_csv("data_input/loan2017Q4.csv")
View(loan2017Q4)
table(loans.s$not_paid) #kalau imbalance, hasil akurasinya bakal tinggi karena datanya gampang di prediksi
prop.table(table(loans.s$not_paid))
summary(loans.s[loans.s$not_paid == 0, "dti"])
summary(loans.s[loans.s$not_paid == 1, "dti"])
xtabs(dti ~ purpose + not_paid, loans.s)
plot(xtabs(dti ~ grade + not_paid, loans.s), main="Assigned Grade of Loan vs Default")
set.seed(417) #set seed agar hasil pemodelannya sama
intrain <- sample(nrow(loans.s), nrow(loans.s)*0.8) #dari 1500 dibuat random sampling, diambil sebanyak 80% dari datanya
loans.train <- loans.s[intrain, ]
loans.test <- loans.s[-intrain, ]
creditrisk <- glm(not_paid ~ verified + purpose + installment + int_rate + home_ownership + grdCtoA + annual_inc, loans.train, family="binomial")
summary(creditrisk)
install.packages("mice")
set.seed(417) #set seed agar hasil pemodelannya sama
intrain <- sample(nrow(loans.s), nrow(loans.s)*0.8) #dari 1500 dibuat random sampling, diambil sebanyak 80% dari datanya
loans.train <- loans.s[intrain, ]
loans.test <- loans.s[-intrain, ]
set.seed(417) #set seed agar hasil pemodelannya sama
intrain <- sample(nrow(loans.s), nrow(loans.s)*0.8) #dari 1500 dibuat random sampling, diambil sebanyak 80% dari datanya
loans.train <- loans.s[intrain, ]
loans.test <- loans.s[-intrain, ]
creditrisk <- glm(not_paid ~ verified + purpose + installment + int_rate + home_ownership + grdCtoA + annual_inc, loans.train, family="binomial")
summary(creditrisk)
creditrisk <- glm(not_paid ~ verified + purpose + installment + int_rate + home_ownership + grdCtoA + annual_inc, loans.train, family="binomial")
summary(creditrisk)
loans.test$pred.Risk <- predict(creditrisk, loans.test, type = "response") #akan menghitung log (odds) lalu akan menghasilkan nilai P
head(loans.test$pred.Risk)
predict(creditrisk, head(loans.test), type = "response")
predict(creditrisk, head(loans.test), type = "link") #default dari predict akan otomatis menggunakan link
predict(creditrisk, head(loans.test))
hist(loans.test$pred.Risk, breaks=20)
table("predicted"=as.numeric(loans.test$pred.Risk>=0.5), "actual"=loans.test$not_paid) #apabila peluangnya kurang dai 0.5, akan dilabeli paid (kelas 0)
table("predicted"=as.numeric(loans.test$pred.Risk>=0.5), "actual"=loans.test$not_paid)
accu <- round((93+97)/nrow(loans.test),2) #(TP+TN)/(TP+TN+FP+FN)
reca <- round(97/(54+97),2) #TP/(TP+FN)
prec <- round(97/(68+97),2) #TP/(FP+TP)
spec <- round(93/(93+68),2) #TN/(TN+FP)
paste("Recall:", reca) # Disebut juga true positive rate. untuk klasifikasi data cancer, lebih bagus  nilai recall nya yang besar. Recall digunakan apabila ingin mengambil langkah preventif
paste("Precision:", prec) #Disebut juga true negative rate. precision bagus digunakan untuk spam classification. Aoabila tujuan perusahaan adalah ingin nge-predict nasabah yang penting, maka lebih memilih precision
paste("Specificity:", spec)
paste("Accuracy:", accu)
prop.table(table(loans.test$not_paid))
table("predicted"=as.numeric(loans.test$pred.Risk>=0.4), "actual"=loans.test$not_paid)
library(ROCR)
pred.rocrtest<-prediction(loans.test$pred.Risk, loans.test$not_paid)
plot(performance(pred.rocrtest,"tpr","fpr"))
accu <- round((93+97)/nrow(loans.test),2) #(TP+TN)/(TP+TN+FP+FN)
reca <- round(97/(54+97),2) #TP/(TP+FN)
prec <- round(97/(68+97),2) #TP/(FP+TP)
spec <- round(93/(93+68),2) #TN/(TN+FP)
paste("Recall:", reca) # Disebut juga true positive rate. untuk klasifikasi data cancer, lebih bagus  nilai recall nya yang besar. Recall digunakan apabila ingin mengambil langkah preventif
paste("Precision:", prec) #Disebut juga true negative rate. precision bagus digunakan untuk spam classification. Aoabila tujuan perusahaan adalah ingin nge-predict nasabah yang penting, maka lebih memilih precision
paste("Specificity:", spec)
paste("Accuracy:", accu)
prop.table(table(loans.test$not_paid))
# Create our dataset
food <- data.frame(list(c("apple", "bacon", "banana", "carrot","celery", "cheese","cucumber", "fish", "grape", "green bean", "lettuce", "nuts", "pear", "shrimp","orange"), c(10,1,10,6,3,1,2,3,10,3,1,3,10,2,9), c(9,4,1,10,10,1,8,2,5,7,10,5,7,2,3), c("fruit", "protein", "fruit", "vegetable", "vegetable", "protein", "vegetable", "protein", "fruit", "vegetable", "vegetable", "proteins", "fruit","protein", "fruit")))
# Give each feature appropriate names
colnames(food)<- c("Ingredient", "Sweetness", "Crunchiness", "Type")
library(ggplot2)
plot.fruit <- ggplot(food, aes(x=food$Sweetness, y=food$Crunchiness))+geom_point(alpha = 0.05)+geom_label(aes(label=food$Ingredient),nudge_x=0.5, nudge_y=0, label.size=0.6)+labs(x="how sweet the food tastes", y="how crunchy the food is")
plot.fruit
library(grid)
grob = grobTree(textGrob("tomato", x=0.6, y=0.4, hjust=0, gp=gpar(col="darkorange", fontsize=14)))
plot.fruit + annotation_custom(grob)
# importing the data and quickly explore the first 5 variables of the dataset
wbcd <- read.csv("data_input/wisc_bc_data.csv", stringsAsFactors = FALSE)
str(wbcd[,1:5])
# importing the data and quickly explore the first 5 variables of the dataset
wbcd <- read.csv("data_input/wisc_bc_data.csv", stringsAsFactors = FALSE)
str(wbcd[,1:5])
wbcd <- wbcd[,-1]
wbcd$diagnosis <- factor(wbcd$diagnosis, levels = c("B", "M"), labels = c("Benign", "Malignant"))
table(wbcd$diagnosis)
round(prop.table(table(wbcd$diagnosis))*100,1)
# Find any patterns of relationship between our 2nd and 6th features
pairs(wbcd[,2:6]) #kalau ada hubungan variable nya ada kecenderungan linear, maka kita bisa memilih salah satu variable karena satu variable sudah menggambarkakn variable yang lain
cor(wbcd[,2:6])
# Creating a normalize() function, which takes a vector x and for each value in that vector, subtracts the minimum value in x and divides by the range of x
normalize <- function(x){
return (
(x - min(x))/(max(x) - min(x))
)
}
normalize(c(1,2,3,4,5))
normalize(c(2000, 2500, 3000, 2800))
# we also drop the diagnosis outcome variable in the new dataframe, hence [2:31]
wbcd_n <- as.data.frame(lapply(wbcd[,2:31], normalize))
range(wbcd[,2:31])
range(wbcd_n[,2:30])
wbcd_train <- wbcd_n[1:469, ]
wbcd_test <- wbcd_n[470:569, ]
# remember we excluded our target variable (diagnosis) in wbcd_n
# here we create our labels vector for use in training the knn model later
wbcd_train_labels <- wbcd[1:469,1]
wbcd_test_labels <- wbcd[470:569,1]
library(class)
# We use k=21, ~sqrt(469) as its also an odd number we eliminate the prob. of getting a tie
wbcd_pred <- knn(train = wbcd_train, test=wbcd_test, cl=wbcd_train_labels, k=21)
table(wbcd_pred)
head(wbcd)
# importing the data and quickly explore the first 5 variables of the dataset
wbcd <- read.csv("data_input/wisc_bc_data.csv", stringsAsFactors = FALSE)
str(wbcd[,1:5])
head(wbcd)
# importing the data and quickly explore the first 5 variables of the dataset
wbcd <- read.csv("data_input/wisc_bc_data.csv", stringsAsFactors = FALSE)
str(wbcd[,1:10])
head(wbcd)
# Creating a normalize() function, which takes a vector x and for each value in that vector, subtracts the minimum value in x and divides by the range of x
normalize <- function(x){
return (
(x - min(x))/(max(x) - min(x))
)
}
normalize(c(1,2,3,4,5))
normalize(c(2000, 2500, 3000, 2800))
# we also drop the diagnosis outcome variable in the new dataframe, hence [2:31]
wbcd_n <- as.data.frame(lapply(wbcd[,2:31], normalize))
wbcd_train <- wbcd_n[1:469, ]
wbcd_test <- wbcd_n[470:569, ]
# remember we excluded our target variable (diagnosis) in wbcd_n
# here we create our labels vector for use in training the knn model later
wbcd_train_labels <- wbcd[1:469,1]
wbcd_test_labels <- wbcd[470:569,1]
library(class)
# We use k=21, ~sqrt(469) as its also an odd number we eliminate the prob. of getting a tie
wbcd_pred <- knn(train = wbcd_train, test=wbcd_test, cl=wbcd_train_labels, k=21)
temp1 <- data.frame(cbind(wbcd_pred, wbcd_test_labels, wbcd_test[,1:3]))
head(temp1,10)
library(class)
# We use k=21, ~sqrt(469) as its also an odd number we eliminate the prob. of getting a tie
wbcd_pred <- knn(train = wbcd_train, test=wbcd_test, cl=wbcd_train_labels, k=21)
table(wbcd_pred)
library(gmodels)
CrossTable(x=wbcd_test_labels, y=wbcd_pred, dnn=c("actual", "prediction"))
temp1 <- data.frame(cbind(wbcd_pred, wbcd_test_labels, wbcd_test[,1:3]))
head(temp1,10)
temp1[temp1$wbcd_pred != wbcd_test_labels, ]
wholesale$Industry <- factor(wholesale$Channel, levels = c(1, 2), labels = c("horeca", "retail"))
# Read the dataset in, drop the "Region" feature because it's not interesting
wholesale <- read.csv("data_input/wholesale.csv", header=TRUE)
wholesale <- wholesale[,-2]
wholesale$Industry <- factor(wholesale$Channel, levels = c(1, 2), labels = c("horeca", "retail"))
# After doing that we can remove the original Channel feature
wholesale <- wholesale[,-1]
table(wholesale$Industry)
library(gmodels)
CrossTable(x=wbcd_test_labels, y=wbcd_pred, dnn=c("actual", "prediction"))
honors.logit4 <- glm(hon ~ math + female + read, data=honors, family="binomial")
honors.logit4 <- glm(hon ~ math + female + read, data=honors, family="binomial")
horeca_pred <- knn(train = wholesale_train[,1:6], test=wholesale_test[,1:6], cl = wholesale_train$Industry, k=15)
exp(honors.logitGrade$coefficients["(Intercept)"])
readFemale<- -11.77025 + 0.97995*1
readMale<- -11.77025 + 0.97995*0
exp(readFemale) / exp(readMale)
exp(readFemale - -readMale)
exp(honors.logitGrade$coefficients[2])
honors.logit4 <- glm(hon ~ math + female + read, data=honors, family="binomial")
summary(honors.logit4)
CrossTable(x = wholesale_test$Industry, y = wholesale_test$V8)
# Read the dataset in, drop the "Region" feature because it's not interesting
wholesale <- read.csv("data_input/wholesale.csv", header=TRUE)
wholesale <- wholesale[,-2]
wholesale$Industry <- factor(wholesale$Channel, levels = c(1, 2), labels = c("horeca", "retail"))
# After doing that we can remove the original Channel feature
wholesale <- wholesale[,-1]
table(wholesale$Industry)
wholesale.z <- as.data.frame(scale(wholesale[,-7]))
summary(wholesale.z)
wholesale.n <- as.data.frame(cbind(wholesale.z, Industry = wholesale$Industry))
# A quick head() shows that the data is shuffled so we don't end up generating a test set comprising of all horeca examples and 0 retail examples
# But we can randomize the ordering if we're uncertain
set.seed(10)
wholesale.n <- wholesale.n[sample(nrow(wholesale.n)), ]
# 80% train - 20% test
wholesale_train <- wholesale.n[1:352, ]
wholesale_test <- wholesale.n[353:440, ]
table(wholesale_train$Industry)
# a and b are any two examples (observations)
euclidean.d <- function(a,b){
# initialize d
d <- 0
# Each a/b has 7 features, so i loops through 1:6 feature & ignores Industry (7th)
for (i in c(1: (length(a)-1) )){
d <- d + ( a[[i]] - b[[i]] )^2
}
d <- sqrt(d)
return(d)
}
knn_classifier <- function( train, test, k){
# initialize a vector that will hold our prediction values
pred.v <- c()
# for each record of test data
for( i in c(1: nrow(test))){
# initialize distance vector, categories
dist.v <- c()
catg.v <- c()
# loop over each train data (think: apple, lettuce, fish)
for(j in c(1:nrow(train))){
# add euclidean distance btw test and train data to dist vec
dist.v <- c(dist.v, euclidean.d(train[j, ], test[i, ]))
# add class variable of training data (apple, lettuce, fish) to categories vec
catg.v <- c(catg.v, as.character(train[j, ][[7]]) )
}
# create a df combining both dist.v and catg.v
neighbors <- data.frame(catg.v, dist.v)
# sort neighbors df so top neighbors are on top
neighbors <- neighbors[order(neighbors[,2]),]
# take the top k neighbors
neighbors <- neighbors[1:k,]
# determine the output and add this to predictions vector
if(nrow(neighbors[neighbors[,1] == "horeca", ]) > nrow(neighbors[neighbors[,1] == "retail", ])){pred.v <- c(pred.v, "horeca")
}else pred.v <- c(pred.v, "retail")
}
return(pred.v)
}
accuracy <- function(data){
# initialize number of predictions
correct <- 0
for(i in c(1:nrow(data))){
#7th variable is actual class, 8th is our predicted class
if(data[i, 7] == data[i, 8]){
correct <- correct + 1
}
}
percentage <- correct/nrow(data) *100
return(percentage)
}
predictions <- knn_classifier(train = wholesale_train, test = wholesale_test, k=19)
wholesale_test[,8] <- predictions
print(accuracy(wholesale_test))
CrossTable(x = wholesale_test$Industry, y = wholesale_test$V8)
names(wholesale_test)[8] <- "ManualKNN"
exp(honors.logitGrade$coefficients["(Intercept)"])
horeca_pred <- knn(train = wholesale_train[,1:6], test=wholesale_test[,1:6], cl = wholesale_train$Industry, k=15)
##library(gmodels)
##CrossTable(x=wholesale_test[,7], y=horeca_pred, dnn=c("actual", "prediction"))
table(horeca_pred, wholesale_test$Industry)
honors <- read.csv("data_input/sample.csv")
honors$hon <- as.factor(honors$hon)
prop.table(table(honors$hon))
str(honors)
honors.logit <- glm(hon ~ 1, data=honors, family="binomial") #family binomial digunakan karena distribusi binomial ingin menghitung probability dari variable dengan 2 kelas
summary(honors.logit) #coefficient menunjukkan hasil estimasi beta0, beta1. null deviance menunjukkan seberapa bagus hasil dari model yang dibangun.
#intrepretasi hasil di bawah: dari nilai nilai intercept -1.1255 yang menunjukkan bahwa odds nya adalah 0.3244, yang artinya probabilitas dia tidak dapat honor lebih besar dibandingkan peluang dia dapat honor
honors.logit <- (glm(hon ~1, data = honors, family = "binomial"))
summary(honors.logit)
honors.logit4 <- glm(hon ~ math + female + read, data=honors, family="binomial")
honors.logit2 <- glm(hon ~ female, data=honors, family="binomial")
paste("Male:", (17/91)/(74/91))
paste("Female:", (32/109)/(77/109))
-1.50792 - (-1.66426)
# 0.15634, the diff with a one-unit increase should also be the
# coefficient for our math variable
honors.logit3$coefficients[2]
exp(honors.logit3$coefficients["(Intercept)"])
honors.logit3 <- glm( hon ~ math, data=honors, family="binomial" )
summary(honors.logit3)
exp(read51) / exp(read50)
log(0.2297297) #kemungkinan male mendapatkan honor lebih kecil dari kemungkinan dia ga dapat honor. kalau coefficient negatif, berarti kemungkinan dia dapat honor itu kecil. kalau positif,berarti kemungkinan male dapat honor itu
log(1.809)
# calculating the change in odds
# odds of female in honors / odds of male in honors
# log of odds to odds is an exponent transformation
exp(-0.8781)/exp(-1.4709)
exp(honors.logit2$coefficients["female"])
#change in odd tiap kenaikan 1 nilai reader
honors.logit4 <- glm( hon ~ read, data=honors, family="binomial" )
summary(honors.logit4)
exp(honors.logit4$coefficients["(Intercept)"])
read50<- -7.58690 + 0.11763*50
read51<- -7.58690 + 0.11763*51
exp(read51) / exp(read50)
exp(read51 - -read50)
exp(0.11763)
exp(honors.logit4$coefficients[2])
honors.logit3 <- glm( hon ~ math, data=honors, family="binomial" )
summary(honors.logit3)
exp(honors.logit3$coefficients["(Intercept)"])
hist(honors$math, breaks=20)
abline(v=median(honors$math), col="blue", lwd=3)
-1.50792 - (-1.66426)
# 0.15634, the diff with a one-unit increase should also be the
# coefficient for our math variable
honors.logit3$coefficients[2]
exp(-1.50792) / exp(-1.66426)
exp(-1.50792 - -1.66426)
exp(0.15634)
exp(honors.logit3$coefficients[2])
honors.logit5 <- glm(hon ~ math + female + read, data=honors, family="binomial")
summary(honors.logit5)
exp(0.05906)
